<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>RobotMotion Dataset</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <div class="container">

    <!-- HERO -->
    <section class="hero">
  <div class="hero-content">
    <h1>RobotMotion</h1>
    <p class="subtitle">A large-scale human behavior dataset for training physical AI and humanoid robots.</p>

    <div class="links">
  <a
    href="https://drive.google.com/file/d/1X11gjIQFWZTJ8rESoPLEWf8hzI-FwM7z/view?usp=sharing"
    target="_blank"
    class="button"
  >
    Paper
  </a>

  <a
    href="https://drive.google.com/file/d/1SGuQxCZZ-zgUALXhXhfcWfVtf8GkS6of/view?usp=sharing"
    target="_blank"
    class="button"
  >
    Dataset
  </a>
  <a
    href="https://drive.google.com/file/d/1nThpJsG2QF7CbkWY1zUlkWKuPV2PG3_5/view?usp=sharing"
    target="_blank"
    class="button"
  >
    Timeline
  </a>
  <a
    href="https://drive.google.com/file/d/12MPpdxoN9DIZZ_JtEwjZCzHr8MA-btSi/view?usp=sharing"
    target="_blank"
    class="button"
  >
    Roadmap
  </a>
</div>
  </div>
</section>

    <!-- ABOUT -->
    <section class="minititle">
  <h2>Abstract</h2>
  <p>
    RobotMotion is a large-scale dataset of real-world human behavior designed to
    support research in robot learning and physical AI. The dataset captures
    egocentric human demonstrations across diverse tasks, environments, and
    object configurations, with a focus on long-horizon, multi-step activities.
    RobotMotion aims to bridge the gap between human behavior data and embodied
    robotic systems.
  </p>
</section>

    <!-- Project objectives -->
    <section id="objectives">
  <h2>Project Objectives</h2>
  <p>
    RobotMotion aims to build a large-scale, high-quality, multi-modal humanoid dataset and
    an associated AI development pipeline to accelerate research and real-world deployment
    of humanoid robots across perception, reasoning, and action. The project focuses on
    enabling learning for vision-language-action models, manipulation, locomotion, and
    embodied decision-making in diverse real-world environments.
  </p>
</section>


        <!-- Project status -->
    <section class="status">
  <h2>Project Status</h2>

  <p class="status-note">
    RobotMotion is an ongoing dataset and system development effort.
    The metrics below reflect the current internal progress and will
    be updated as the project evolves.
  </p>

  <div class="status-grid">
    <div class="status-item">
      <span class="status-value">12</span>
      <span class="status-label">Skills defined</span>
    </div>

    <div class="status-item">
      <span class="status-value">140</span>
      <span class="status-label">Recorded episodes</span>
    </div>

    <div class="status-item">
      <span class="status-value">~3.5 hrs</span>
      <span class="status-label">Multi-modal data</span>
    </div>

    <div class="status-item">
      <span class="status-value">3</span>
      <span class="status-label">Data sources</span>
    </div>

    <div class="status-item">
      <span class="status-value">RGB / RGB-D</span>
      <span class="status-label">Vision modalities</span>
    </div>

    <div class="status-item">
      <span class="status-value">Human / Sim / TeleOp</span>
      <span class="status-label">Collection modes</span>
    </div>
  </div>
</section>

<!-- Dataset overview -->
    <section>
      <section>
  <h2>Dataset Examples </h2>

  <section class="paired-demo">

  <p class="task-title">
    <em>Trained robot hands doing multiple task.</em>
  </p>

  <div class="paired-videos">

    <!-- Left video -->
    <div class="demo-video">
      <video autoplay loop muted playsinline>
        <source src="assets/videos/Imr Cobot.mp4" type="video/mp4">
      </video>
      <p class="demo-caption">Pick and place an object</p>
    </div>

    <!-- Right video (new one) -->
    <div class="demo-video">
      <video autoplay loop muted playsinline>
        <source src="assets/videos/Vulcan X Vinrobotics Mp4.mp4" type="video/mp4">
      </video>
      <p class="demo-caption">And multiple other tasks</p>
    </div>

  </div>

</section>


  
</section>


</section>

    
  <section>
  <h2>Dataset Overview</h2>

  <p>
    RobotMotion is a large-scale dataset of real-world human behavior designed
    for research in robot learning and physical AI. The dataset captures
    egocentric demonstrations of task-oriented activities performed in
    everyday environments, with an emphasis on long-horizon, multi-step actions.
  </p>

  <ul class="dataset-spec">
    <li>
      <strong>Modalities:</strong>
      RGB video, optional audio, task-level metadata
    </li>
    <li>
      <strong>Viewpoint:</strong>
      Egocentric (first-person), aligned with human perception
    </li>
    <li>
      <strong>Environments:</strong>
      Indoor and real-world daily settings with diverse object configurations
    </li>
    <li>
      <strong>Temporal Structure:</strong>
      Continuous sequences capturing full task execution
    </li>
    <li>
      <strong>Granularity:</strong>
      Long-horizon action sequences rather than isolated primitives
    </li>
  </ul>

    <img
    src="assets/image/RM AI Infrastructure.PNG"
    alt="Overview of RobotMotion dataset structure"
    class="overview-diagram"
  />
    <p class="diagram-caption">
    Overview of the RobotMotion data pipeline, illustrating the flow
    between Human videos, Simulation Data, and TeleOps.
  </p>
</section>

    <!-- Data Sources -->
    <section id="data-sources">
  <h2>Data Sources</h2>
  <ul>
    <li><strong>Human Data:</strong> Egocentric and third-person human demonstrations collected in natural, unconstrained environments.</li>
    <li><strong>Simulation Data:</strong> Large-scale data generated in physics-based simulators with rich state information and ground-truth annotations.</li>
    <li><strong>Teleoperation Data:</strong> Real humanoid robot executions captured via human-in-the-loop control.</li>
  </ul>
</section>

<!-- Data modalities -->
  <section>
  <h2>Data Modalities</h2>

  <p>
    RobotMotion records multi-modal observations of real-world human behavior.
    The listed modalities describe the signals provided by the dataset and may
    be selectively used, downsampled, or adapted depending on the target robot
    platform and learning method.
  </p>

  <table class="modality-table">
    <thead>
      <tr>
        <th>Modality</th>
        <th>Description</th>
        <th>Temporal Resolution</th>
        <th>Availability</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>RGB video</td>
        <td>Egocentric, first-person visual observations</td>
        <td>10–30 Hz</td>
        <td>All recordings</td>
      </tr>
      <tr>
        <td>Audio</td>
        <td>Ambient and task-related audio signals</td>
        <td>Optional</td>
        <td>Subset of recordings</td>
      </tr>
      <tr>
        <td>Task metadata</td>
        <td>Task labels, episode boundaries, and annotations</td>
        <td>Per episode</td>
        <td>All recordings</td>
      </tr>
      <tr>
        <td>Temporal segmentation</td>
        <td>Action and sub-task boundaries</td>
        <td>Variable</td>
        <td>Subset of recordings</td>
      </tr>
      <tr>
        <td>Environment context</td>
        <td>Scene and object-level context information</td>
        <td>Per episode</td>
        <td>All recordings</td>
      </tr>
      <tr>
        <td>Robot execution data</td>
        <td>Robot actions aligned to human demonstrations</td>
        <td>10–100 Hz</td>
        <td>When available</td>
      </tr>
    </tbody>
  </table>
</section>

    <section class="digital-twins">
  <h2>Digital Twins</h2>

  <p>
    RobotMotion enables the construction of simulation-aligned digital twins
    derived from real-world human demonstrations. These digital twins represent
    task execution as structured, temporally coherent sequences that can be
    replayed, analyzed, or used for learning in simulated environments.
  </p>

  <p>
    Rather than aiming for exact physical replication, RobotMotion focuses on
    capturing task-relevant structure, object interactions, and temporal context
    that are transferable from real-world demonstrations to simulation.
  </p>

  <!-- GIF goes here -->
  <div class="twin-visual-grid">

  <!-- Left: real-world demonstration -->
  <div class="twin-item">
    <video autoplay loop muted playsinline>
      <source src="assets/videos/digital_twins1.mp4" type="video/mp4">
    </video>
    <p class="twin-caption">Real-world machine deployment</p>
  </div>

  <!-- Right: digital twin / simulation -->
  <div class="twin-item">
    <video autoplay loop muted playsinline>
      <source src="assets/videos/digital_twins2.mp4" type="video/mp4">
    </video>
    <p class="twin-caption">Warehouse digital twin & pathfinding</p>
  </div>

</div>


  <p class="note">
    Digital twins in RobotMotion are designed to capture task-relevant structure
    rather than exact physical or geometric replication, and may vary in fidelity
    depending on the task and environment.
  </p>
</section>

    <section class="samples">
  <h2>Samples</h2>

  <p class="samples-desc">
    The following videos show representative samples from the RobotMotion dataset,
    illustrating demonstrations across different tasks and small tools manipulation.
  </p>

  <div class="samples-grid">

    <div class="sample-item">
      <video autoplay loop muted playsinline>
        <source src="assets/videos/Small object manipulation.mp4" type="video/mp4">
      </video>
      <p class="sample-caption">Wielding small chip</p>
    </div>

    <div class="sample-item">
      <video autoplay loop muted playsinline>
        <source src="assets/videos/Tool manipulation.mp4" type="video/mp4">
      </video>
      <p class="sample-caption">Tools manipulation</p>
    </div>

  </div>
</section>


    <!-- Scale & Statistics -->
    <section>
  <h2>Scale & Statistics</h2>

  <ul class="dataset-spec">
    <li>
      <strong>Total recordings:</strong>
      Tens of thousands of task episodes (ongoing collection)
    </li>
    <li>
      <strong>Total duration:</strong>
      100+ hours of egocentric human demonstrations
    </li>
    <li>
      <strong>Number of tasks:</strong>
      20+ task categories spanning manipulation and daily activities
    </li>
    <li>
      <strong>Number of environments:</strong>
      Multiple real-world indoor environments
    </li>
    <li>
      <strong>Temporal structure:</strong>
      Continuous, long-horizon task execution
    </li>
  </ul>

  <p>
    The dataset will continue to expand over time as new tasks
    and environments are added.
  </p>
</section>




    <!-- DATASET -->
    <section>
      <h2>Dataset</h2>
      <ul>
        <li>Egocentric human video</li>
        <li>Audio + action context</li>
        <li>Task-annotated behavior sequences</li>
        <li>Multi-environment recordings</li>
      </ul>
    </section>

    <!-- TASKS -->
    <section>
      <h2>Supported Tasks</h2>
      <p>
        Manipulation, navigation, daily activities, tool usage, and
        long-horizon task execution.
      </p>
    </section>

        <!-- Data Access -->
    <section>
  <h2>Data Access</h2>
  <p>
    The RobotMotion dataset will be released in phases.
  </p>

  <ul>
    <li><strong>Phase 1:</strong> Public subset for research and benchmarking</li>
    <li><strong>Phase 2:</strong> Extended dataset under research agreement</li>
    <li><strong>Phase 3:</strong> Commercial access for enterprise use</li>
  </ul>
</section>
    
    <!-- Intended Use -->
<section>
  <h2>Intended Use</h2>
  <p>
    RobotMotion is designed for research in robot learning, imitation learning,
    and physical AI. It is not intended for surveillance or biometric identification.
  </p>
</section>

    <!-- CITATION -->
    <section>
      <h2>Citation</h2>
      <pre>
@misc{robotmotion2025,
  title={RobotMotion: A Human Behavior Dataset for Physical AI},
  author={RobotMotion Team},
  year={2025}
}
      </pre>
      <section>
  <h2>License</h2>
  <p>
    This dataset is licensed under <strong>CC BY-SA 4.0</strong>  
    (or your own license here if different).
  </p>
</section>

    </section>

    <!-- FOOTER -->
    <footer>
      <p>
        Contact: <a href="mailto:contact@robotmotion.ai">contact@robotmotion.ai</a>
      </p>
    </footer>

  </div>
</body>
</html>
